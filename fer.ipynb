{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-517a320534a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import torchvision.datasets as dsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import torchvision.transforms as transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;31m# Shared memory manager needs to know the exact location of manager executable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initExtension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmanager_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0m_queued_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Don't store the actual traceback to avoid memory cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0m_queued_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdatecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclearcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mupdatecache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_buffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mdetect_encoding\u001b[0;34m(readline)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mbom_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mread_or_stop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 300\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomSizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "#                          std  = [ 0.229, 0.224, 0.225 ]),\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "# with open('./ck+.pkl', 'rb') as f:\n",
    "#      data = pickle.load(f)\n",
    "# print('load done')     \n",
    "\n",
    "# index = [i for i in range(927)]\n",
    "# random.shuffle(index)\n",
    "# trainindex = index[:835]\n",
    "# testindex = index[835:]\n",
    "\n",
    "# # dic = {'angry':0, 'disgust':1, 'fear':2, 'happy':3, 'neutral':4, 'sad':5, 'surprise':6}\n",
    "# dic = {'angry':0, 'disgusted':1, 'fearful':2, 'happy':3, 'sadness':4, 'surprised':5}\n",
    "# for i in range(len(data[1])):\n",
    "#     data[1][i] = dic[data[1][i]]\n",
    "# testlabel = np.array(data[1])[testindex]\n",
    "# trainlabel = np.array(data[1])[trainindex]\n",
    "# testdata = np.array(data[3])[testindex]\n",
    "# traindata = np.array(data[3])[trainindex]\n",
    "\n",
    "# trainData = TensorDataset(torch.Tensor(traindata), torch.LongTensor(trainlabel))\n",
    "# testData = TensorDataset(torch.Tensor(testdata), torch.LongTensor(testlabel))\n",
    "# print('to tensor done')\n",
    "\n",
    "# trainLoader = DataLoader(dataset=trainData, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "# print('dataloader done')\n",
    "\n",
    "with open('./raf_train.pkl', 'rb') as f:\n",
    "     train_data = pickle.load(f)\n",
    "print('load train done')\n",
    "\n",
    "# with open('./raf_test.pkl', 'rb') as f:\n",
    "#      test_data = pickle.load(f)\n",
    "# print('load test done')\n",
    "\n",
    "\n",
    "trainData = TensorDataset((torch.Tensor(train_data[2]))[:, :10], torch.LongTensor(train_data[0]))\n",
    "# testData = TensorDataset(torch.Tensor(test_data[2]), torch.LongTensor(test_data[0]))\n",
    "print('to tensor done')\n",
    "\n",
    "trainLoader = DataLoader(dataset=trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "print('dataloader done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test done\n",
      "to tensor done\n",
      "dataloader done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "with open('./raf_test.pkl', 'rb') as f:\n",
    "     test_data = pickle.load(f)\n",
    "print('load test done')\n",
    "\n",
    "testData = TensorDataset((torch.Tensor(test_data[2]))[:, :10], torch.LongTensor(test_data[0]))\n",
    "# testData = TensorDataset(torch.Tensor(test_data[2]), torch.LongTensor(test_data[0]))\n",
    "print('to tensor done')\n",
    "\n",
    "\n",
    "testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "print('dataloader done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor([[[1,2],[1,2]],[[4,5],[4,5]],[[7,8],[7,8]]])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5000]],\n",
      "\n",
      "        [[4.5000]],\n",
      "\n",
      "        [[7.5000]]]) torch.Size([3, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5000,  0.5000],\n",
       "         [-0.5000,  0.5000]],\n",
       "\n",
       "        [[-0.5000,  0.5000],\n",
       "         [-0.5000,  0.5000]],\n",
       "\n",
       "        [[-0.5000,  0.5000],\n",
       "         [-0.5000,  0.5000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(torch.mean(a,axis=1),axis=1).reshape(3,1,1)\n",
    "print(mean, mean.size())\n",
    "a-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./RAFDB/resnetepoch99.pkl')\n",
    "a = checkpoint['net']\n",
    "new_check = OrderedDict()\n",
    "for k in list(a.keys()):   \n",
    "    newk = k.strip('module.') \n",
    "    # print(newk, k)\n",
    "    a[newk] = a.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(64, 128, 1)\n",
    "        self.conv8 = nn.Conv2d(128, 256, 1)\n",
    "        self.BN1 = nn.BatchNorm2d(64)\n",
    "        self.BN2 = nn.BatchNorm2d(128)\n",
    "        self.BN3 = nn.BatchNorm2d(256)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.avepool = nn.MaxPool2d(24, 24)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(2560, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape ==(-1,10,96,96,3)\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            #print(k)\n",
    "            f = x[:,k,:,:,:]\n",
    "            f = f.permute(0,3,1,2) \n",
    "            #（-1，3，96，96）\n",
    "            f1 = F.relu(self.BN1(self.conv0(f))) \n",
    "            #（-1，64，96，96）\n",
    "            f2 = F.relu(self.BN1(self.conv1(f1)))\n",
    "            f3 = self.maxpool(F.relu(f1 + self.BN1(self.conv2(f2))))\n",
    "            residual1 = self.BN2(self.conv7(f3)) # (-1, 128, 48, 48)\n",
    "            # (-1, 64, 48, 48)\n",
    "            f4 = F.relu(self.BN2(self.conv3(f3))) \n",
    "            f5 = self.maxpool(F.relu(residual1 + self.BN2(self.conv4(f4))))\n",
    "            residual2 = self.BN3(self.conv8(f5)) # (-1, 256, 24, 24)    \n",
    "            # (-1, 128, 24, 24)\n",
    "            f6 = F.relu(self.BN3(self.conv5(f5))) \n",
    "            f7 = F.relu(residual2 + self.BN3(self.conv6(f6)))\n",
    "            f8 = self.avepool(f7)  \n",
    "            # (-1, 256, 1, 1)\n",
    "            f8 = f8.view(f8.size(0),-1)\n",
    "            # (-1, 256)\n",
    "            f9 = F.relu(self.fc1(f8))\n",
    "            # (-1, 256)\n",
    "            out.append(f9)\n",
    "        out = torch.cat(out,axis=1) \n",
    "        #  (-1, 256*10)\n",
    "        out = F.relu(self.fc2(self.dropout(out)))\n",
    "        # (-1, 1024)\n",
    "        out = F.softmax(self.fc3(out), dim=1)\n",
    "        # (-1, 7)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG16:\n\tUnexpected key(s) in state_dict: \"BN1.num_batches_track\", \"BN2.num_batches_track\", \"BN3.num_batches_track\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f699d3724d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnew_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# checkpoint = torch.load('./RAFDB/vgg_dropout_epoch349.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG16:\n\tUnexpected key(s) in state_dict: \"BN1.num_batches_track\", \"BN2.num_batches_track\", \"BN3.num_batches_track\". "
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "vgg16 = VGG16()\n",
    "checkpoint = torch.load('./RAFDB/resnetepoch99.pkl')\n",
    "# new_check = OrderedDict()\n",
    "# for k,v in checkpoint.items():   \n",
    "#     newk = k.strip('module.') \n",
    "#     # print(newk, k)\n",
    "#     new_check[newk] = v\n",
    "# checkpoint = torch.load('./RAFDB/vgg_dropout_epoch349.pkl')\n",
    "vgg16.load_state_dict(checkpoint['net'])\n",
    "vgg16.cuda()\n",
    "\n",
    "# Test the model\n",
    "vgg16.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in testLoader:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = vgg16(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv8): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (avepool): MaxPool2d(kernel_size=24, stride=24, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4742,  2.4438, -1.6943, -0.0654,  0.2538,  0.3002,  0.2470,  0.0383,\n",
      "         0.6833, -1.2546,  1.0609, -0.8963,  0.8332,  2.6125, -0.5219, -0.7294,\n",
      "        -2.2121, -0.4871,  0.0253,  0.3766])\n",
      "tensor([ 0.0000,  0.0000, -3.3885, -0.0000,  0.0000,  0.6003,  0.4941,  0.0767,\n",
      "         1.3666, -0.0000,  0.0000, -0.0000,  0.0000,  5.2249, -1.0437, -0.0000,\n",
      "        -4.4242, -0.9743,  0.0507,  0.7532])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "f = nn.Dropout(0.5)\n",
    "a = Variable(torch.randn(20))\n",
    "print(a)\n",
    "print(f(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "new_check = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    if 'module' not in k:\n",
    "        k = 'module.'+k\n",
    "    else:\n",
    "        k = k.replace('features.module.', 'module.features.')\n",
    "    new_check[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # checkpoint = torch.load('./RAFDB/allepoch159.pkl')\n",
    "# checkpoint['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "\n",
    "            # 1-1 conv layer\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1-2 conv layer\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1 Pooling layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "\n",
    "            # 2-1 conv layer\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2-2 conv layer\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2 Pooling lyaer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "\n",
    "            # 3-1 conv layer\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3-2 conv layer\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3 Pooling layer\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            nn.AvgPool2d(kernel_size=24, stride=24))\n",
    "        \n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "\n",
    "            # 6 Fully connected layer\n",
    "            nn.Linear(256, 256),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "        self.layer7 = nn.Sequential(\n",
    "\n",
    "            # 7 Fully connected layer\n",
    "            nn.Linear(256*10, 1024),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer8 = nn.Sequential(\n",
    "\n",
    "            # 8 output layer\n",
    "            nn.Linear(1024, 7),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape ==(-1,10,96,96,3)\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            #print(k)\n",
    "            f = x[:,k,:,:,:]\n",
    "            f = f.permute(0,3,1,2) \n",
    "            #（-1，3，96，96）\n",
    "            f = self.layer1(f)  \n",
    "            # (-1, 64, 48, 48)\n",
    "            f = self.layer2(f)  \n",
    "            # (-1, 128, 24, 24)\n",
    "            f = self.layer3(f)  \n",
    "            # (-1, 256, 1, 1)\n",
    "            f = f.view(f.size(0),-1)\n",
    "            # (-1, 256)\n",
    "            f = self.layer6(f)\n",
    "            # (-1, 256)\n",
    "            out.append(f)\n",
    "        out = torch.cat(out,axis=1) \n",
    "        #  (-1, 256*10)\n",
    "        out = self.layer7(out)\n",
    "        # (-1, 1024)\n",
    "        out = self.layer8(out)\n",
    "        # (-1, 7)\n",
    "        return out\n",
    "    \n",
    "vgg16 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = []\n",
    "        for i in range(10):\n",
    "            self.layer1.append(nn.Sequential())\n",
    "            self.layer1[i].add_module(str(i)+'conv1', nn.Conv2d(3, 64, kernel_size=3, padding=1))\n",
    "            self.layer1[i].add_module(str(i)+'bn1', nn.BatchNorm2d(64))\n",
    "            self.layer1[i].add_module(str(i)+'relu1',  nn.ReLU())\n",
    "            self.layer1[i].add_module(str(i)+'conv2',  nn.Conv2d(64, 64, kernel_size=3, padding=1))\n",
    "            self.layer1[i].add_module(str(i)+'bn2', nn.BatchNorm2d(64))\n",
    "            self.layer1[i].add_module(str(i)+'relu2',  nn.ReLU())\n",
    "            self.layer1[i].add_module(str(i)+'maxpool1',  nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = []\n",
    "        for i in range(10):\n",
    "            self.layer2.append(nn.Sequential())\n",
    "            self.layer2[i].add_module(str(i)+'conv3', nn.Conv2d(64, 128, kernel_size=3, padding=1))\n",
    "            self.layer2[i].add_module(str(i)+'bn3', nn.BatchNorm2d(128))\n",
    "            self.layer2[i].add_module(str(i)+'relu3',  nn.ReLU())\n",
    "            self.layer2[i].add_module(str(i)+'conv4',  nn.Conv2d(128, 128, kernel_size=3, padding=1))\n",
    "            self.layer2[i].add_module(str(i)+'bn4', nn.BatchNorm2d(128))\n",
    "            self.layer2[i].add_module(str(i)+'relu4',  nn.ReLU())\n",
    "            self.layer2[i].add_module(str(i)+'maxpool2',  nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = []\n",
    "        for i in range(10):\n",
    "            self.layer3.append(nn.Sequential())\n",
    "            self.layer3[i].add_module(str(i)+'conv5', nn.Conv2d(128, 256, kernel_size=3, padding=1))\n",
    "            self.layer3[i].add_module(str(i)+'b53', nn.BatchNorm2d(256))\n",
    "            self.layer3[i].add_module(str(i)+'relu5',  nn.ReLU())\n",
    "            self.layer3[i].add_module(str(i)+'conv6',  nn.Conv2d(256 256, kernel_size=3, padding=1))\n",
    "            self.layer3[i].add_module(str(i)+'bn6', nn.BatchNorm2d(256))\n",
    "            self.layer3[i].add_module(str(i)+'relu6',  nn.ReLU())\n",
    "            self.layer3[i].add_module(str(i)+'avgpool',  nn.AvgPool2d(kernel_size=24, stride=24))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            #print(k)\n",
    "            f = x[:,k,:,:,:]\n",
    "            f = f.permute(0,3,1,2) \n",
    "            f = self.layer1[k](f)  \n",
    "            out.append(f)\n",
    "        out = torch.cat(out,axis=1) \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (0bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (0relu1): ReLU()\n",
       "   (0conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (0bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (0relu2): ReLU()\n",
       "   (0maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (1conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (1relu1): ReLU()\n",
       "   (1conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (1relu2): ReLU()\n",
       "   (1maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (2conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (2bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (2relu1): ReLU()\n",
       "   (2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (2bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (2relu2): ReLU()\n",
       "   (2maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (3conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (3relu1): ReLU()\n",
       "   (3conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (3relu2): ReLU()\n",
       "   (3maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (4conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (4bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (4relu1): ReLU()\n",
       "   (4conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (4bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (4relu2): ReLU()\n",
       "   (4maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (5conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (5bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (5relu1): ReLU()\n",
       "   (5conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (5bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (5relu2): ReLU()\n",
       "   (5maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (6conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (6bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (6relu1): ReLU()\n",
       "   (6conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (6bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (6relu2): ReLU()\n",
       "   (6maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (7conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (7bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (7relu1): ReLU()\n",
       "   (7conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (7bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (7relu2): ReLU()\n",
       "   (7maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (8conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (8bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (8relu1): ReLU()\n",
       "   (8conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (8bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (8relu2): ReLU()\n",
       "   (8maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (9conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (9bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (9relu1): ReLU()\n",
       "   (9conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (9bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (9relu2): ReLU()\n",
       "   (9maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " )]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = VGG16()\n",
    "vgg.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "\n",
    "BATCH_SIZE =32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 300\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomSizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "#                          std  = [ 0.229, 0.224, 0.225 ]),\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "# with open('./ck+.pkl', 'rb') as f:\n",
    "#      data = pickle.load(f)\n",
    "        \n",
    "# trainlabel = data[1][:820] \n",
    "# testlabel = data[1][820:]\n",
    "# # dic = {'angry':0, 'disgust':1, 'fear':2, 'happy':3, 'neutral':4, 'sad':5, 'surprise':6}\n",
    "# dic = {'angry':0, 'disgusted':1, 'fearful':2, 'happy':3, 'sadness':4, 'surprised':5}\n",
    "# for i in range(len(testlabel)):\n",
    "#     testlabel[i] = dic[testlabel[i]]\n",
    "# for i in range(len(trainlabel)):\n",
    "#     trainlabel[i] = dic[trainlabel[i]] \n",
    "    \n",
    "# trainregion = data[3][:820]\n",
    "# testregion = data[3][820:]\n",
    "\n",
    "with open('./raf_train.pkl', 'rb') as f:\n",
    "     train_data = pickle.load(f)\n",
    "trainlabel = train_data[1]\n",
    "trainregion = train_data[3]\n",
    "\n",
    "with open('./raf_test.pkl', 'rb') as f:\n",
    "     test_data = pickle.load(f)\n",
    "testlabel = test_data[1]\n",
    "testregion = test_data[3]\n",
    "\n",
    "trainData = TensorDataset(torch.Tensor(trainregion), torch.LongTensor(trainlabel))\n",
    "testData = TensorDataset(torch.Tensor(testregion), torch.LongTensor(testlabel))\n",
    "\n",
    "trainLoader = DataLoader(dataset=trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = []\n",
    "        for i in range(10):\n",
    "            self.layer1[i] = nn.Sequential()\n",
    "            self.layer1[i].add_module(str(i)+'conv1', nn.Conv2d(3, 64, kernel_size=3, padding=1))\n",
    "            self.layer1[i].add_module(str(i)+'bn1', nn.BatchNorm2d(64))\n",
    "            self.layer1[i].add_module(str(i)+'relu1',  nn.ReLU())\n",
    "            self.layer1[i].add_module(str(i)+'conv2',  nn.Conv2d(64, 64, kernel_size=3, padding=1))\n",
    "            self.layer1[i].add_module(str(i)+'bn2', nn.BatchNorm2d(64))\n",
    "            self.layer1[i].add_module(str(i)+'relu2',  nn.ReLU())\n",
    "            self.layer1[i].add_module(str(i)+'maxpool',  nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "\n",
    "            # 2-1 conv layer\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2-2 conv layer\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2 Pooling lyaer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "\n",
    "            # 3-1 conv layer\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3-2 conv layer\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3 Pooling layer\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            nn.AvgPool2d(kernel_size=24, stride=24))\n",
    "        \n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "\n",
    "            # 6 Fully connected layer\n",
    "            nn.Linear(256, 256),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "        self.layer7 = nn.Sequential(\n",
    "\n",
    "            # 7 Fully connected layer\n",
    "            nn.Linear(256*10, 1024),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer8 = nn.Sequential(\n",
    "\n",
    "            # 8 output layer\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape ==(-1,18,12,12,3)\n",
    "        #print(x.shape)\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            #print(k)\n",
    "            f = x[:,k,:,:,:]\n",
    "            f = f.permute(0,3,1,2) \n",
    "            #（-1，3，96，96）\n",
    "            f = self.layer1(f)  \n",
    "            # (-1, 64, 48, 48)\n",
    "            f = self.layer2(f)  \n",
    "            # (-1, 128, 24, 24)\n",
    "            f = self.layer3(f)  \n",
    "            # (-1, 256, 1, 1)\n",
    "            f = f.view(f.size(0),-1)\n",
    "            # (-1, 256)\n",
    "            f = self.layer6(f)\n",
    "            # (-1, 256)\n",
    "            out.append(f)\n",
    "        out = torch.cat(out,axis=1) \n",
    "        #  (-1, 256*10)\n",
    "        out = self.layer7(out)\n",
    "        # (-1, 1024)\n",
    "        out = self.layer8(out)\n",
    "        # (-1, 6)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "vgg16 = VGG16()\n",
    "vgg16.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.parameters(), lr=LEARNING_RATE)\n",
    "# Train the model\n",
    "for epoch in range(EPOCH):\n",
    "    if epoch > 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001\n",
    "    if epoch > 150:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainLoader):\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        out = vgg16.forward(images)\n",
    "        #print(out.shape,labels.shape)\n",
    "        loss = cost(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    print('epoch: %d' % epoch, loss.data)\n",
    "    print('Test Accuracy of the model on the train images: %d %%' % (100 * correct / total))\n",
    "    if (epoch+1) % 10 == 0 :\n",
    "        torch.save(vgg16.state_dict(), './RAFDB/epoch{}.pkl'.format(epoch))\n",
    "        \n",
    "# Test the model\n",
    "vgg16.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in testLoader:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = vgg16(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "\n",
    "BATCH_SIZE =32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 100\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomSizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "#                          std  = [ 0.229, 0.224, 0.225 ]),\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "with open('./ck+.pkl', 'rb') as f:\n",
    "     data = pickle.load(f)\n",
    "        \n",
    "trainlabel = data[1][:820] \n",
    "testlabel = data[1][820:]\n",
    "# dic = {'angry':0, 'disgust':1, 'fear':2, 'happy':3, 'neutral':4, 'sad':5, 'surprise':6}\n",
    "dic = {'angry':0, 'disgusted':1, 'fearful':2, 'happy':3, 'sadness':4, 'surprised':5}\n",
    "for i in range(len(testlabel)):\n",
    "    testlabel[i] = dic[testlabel[i]]\n",
    "for i in range(len(trainlabel)):\n",
    "    trainlabel[i] = dic[trainlabel[i]] \n",
    "    \n",
    "trainregion = data[3][:820]\n",
    "testregion = data[3][820:]\n",
    "\n",
    "trainData = TensorDataset(torch.Tensor(trainregion), torch.LongTensor(trainlabel))\n",
    "testData = TensorDataset(torch.Tensor(testregion), torch.LongTensor(testlabel))\n",
    "\n",
    "trainLoader = DataLoader(dataset=trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "\n",
    "            # 1-1 conv layer\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1-2 conv layer\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1 Pooling layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "\n",
    "            # 2-1 conv layer\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2-2 conv layer\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2 Pooling lyaer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "\n",
    "            # 3-1 conv layer\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3-2 conv layer\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3 Pooling layer\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            nn.AvgPool2d(kernel_size=24, stride=24))\n",
    "        \n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "\n",
    "            # 6 Fully connected layer\n",
    "            nn.Linear(256, 256),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "        self.layer7 = nn.Sequential(\n",
    "\n",
    "            # 7 Fully connected layer\n",
    "            nn.Linear(256*10, 1024),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer8 = nn.Sequential(\n",
    "\n",
    "            # 8 output layer\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape ==(-1,18,12,12,3)\n",
    "        f = x[:,k,:,:,:]\n",
    "        f = f.permute(0,3,1,2) \n",
    "        #（-1，3，96，96）\n",
    "        f = self.layer1(f)  \n",
    "        # (-1, 64, 48, 48)\n",
    "        f = self.layer2(f)  \n",
    "        # (-1, 128, 24, 24)\n",
    "        f = self.layer3(f)  \n",
    "        # (-1, 256, 1, 1)\n",
    "        f = f.view(f.size(0),-1)\n",
    "        # (-1, 256)\n",
    "        f = self.layer6(f)\n",
    "        # (-1, 256)\n",
    "        return f\n",
    "        \n",
    "\n",
    "\n",
    "vgg16 = []\n",
    "vgg16.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.parameters(), lr=LEARNING_RATE)\n",
    "# Train the model\n",
    "for epoch in range(EPOCH):\n",
    "#     if epoch > 5:\n",
    "#         optimizer.lr = 0.001\n",
    "#         # print(optimizer.lr)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainLoader):\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            vgg16.append(VGG16())\n",
    "            out.append(vgg16[k].forward(images[:, k, :, :])) #(-1, 256)\n",
    "        out = torch.cat(out,axis=1) \n",
    "        #  (-1, 256*10)\n",
    "        out = vgg16[0].layer7(out)\n",
    "        # (-1, 1024)\n",
    "        out = vgg16[0].layer8(out)\n",
    "        # (-1, 6)\n",
    "        # out = vgg16.forward(images)\n",
    "        #print(out.shape,labels.shape)\n",
    "        loss = cost(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    print('epoch: %d' % epoch, loss.data)\n",
    "    print('Test Accuracy of the model on the train images: %d %%' % (100 * correct / total))\n",
    "    # if (epoch+1) % 10 == 0 :\n",
    "        # print('Epoch [%d/%d], Loss. %.4f' %(epoch+1, EPOCH, loss.data[0]))\n",
    "    #torch.save(vgg16.state_dict(), './epoch{}.pkl'.format(epoch))\n",
    "# Test the model\n",
    "vgg16.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in testLoader:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = vgg16(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 1, 1)\n",
      "torch.Size([1, 12, 1, 1])\n",
      "torch.Size([1, 24, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "a = np.ones((1,12,1,1))\n",
    "print(a.shape)\n",
    "res = torch.FloatTensor([])\n",
    "a = torch.FloatTensor(a)\n",
    "b = torch.FloatTensor(np.ones((1,12,1,1)))\n",
    "res = torch.cat((res, a), 1)\n",
    "print(res.shape)\n",
    "res = torch.cat((res, b), 1)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#数据预处理：转换为Tensor，归一化，设置训练集和验证集以及加载子进程数目\n",
    "transform = transforms.Compose([transforms.ToTensor() , transforms.Normalize((0.5 , 0.5 , 0.5) , (0.5 , 0.5 , 0.5))])  #前面参数是均值，后面是标准差\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data' , train = True , download = True , transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset , batch_size = 4 , shuffle = True , num_workers =2) \n",
    "testset = torchvision.datasets.CIFAR10(root = './data' , train = False , download = True , transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset , batch_size = 4 , shuffle = True , num_workers = 2)\n",
    "classes = ('plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg , (1 , 2 , 0)))\n",
    "    pylab.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images , labels = dataiter.next()\n",
    "for i in range(4):\n",
    "    p = plt.subplot()\n",
    "    p.set_title(\"label: %5s\" % classes[labels[i]])\n",
    "    imshow(images[i])\n",
    "#构建网络\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net , self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3 , 6 , 5)\n",
    "        self.pool = nn.MaxPool2d(2 , 2)\n",
    "        self.conv2 = nn.Conv2d(6 , 16 , 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5 , 120)\n",
    "        self.fc2 = nn.Linear(120 , 84)\n",
    "        self.fc3 = nn.Linear(84 , 10)\n",
    "\n",
    "    def forward(self , x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1 , 16 * 5 * 5)  #利用view函数使得conv2层输出的16*5*5维的特征图尺寸变为400大小从而方便后面的全连接层的连接\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "\n",
    "#define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters() , lr = 0.001 , momentum = 0.9)\n",
    "\n",
    "#train the Network\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i , data in enumerate(trainloader , 0):\n",
    "        inputs , labels = data\n",
    "        inputs , labels = Variable(inputs.cuda()) , Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        #forward + backward + optimizer\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs , labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d , %5d] loss: %.3f' % (epoch + 1 , i + 1 , running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images , labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth:' , ' '.join(classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = net(Variable(images.cuda()))\n",
    "\n",
    "_ , predicted = torch.max(outputs.data , 1)\n",
    "print('Predicted: ' , ' '.join('%5s' % classes[predicted[j]] for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images , labels = data\n",
    "    outputs = net(Variable(images.cuda()))\n",
    "    _ , predicted = torch.max(outputs.data , 1)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "    total += labels.size(0)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "class_correct = torch.ones(10).cuda()\n",
    "class_total = torch.ones(10).cuda()\n",
    "for data in testloader:\n",
    "    images , labels = data\n",
    "    outputs = net(Variable(images.cuda()))\n",
    "    _ , predicted = torch.max(outputs.data , 1)\n",
    "    c = (predicted == labels.cuda()).squeeze()\n",
    "    #print(predicted.data[0])\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i] , 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with open('./raf_test_96.pkl', 'rb') as f:\n",
    "     test_data = pickle.load(f)\n",
    "print('load test done')\n",
    "\n",
    "# testData = TensorDataset((torch.Tensor(test_data[2]))[:, :10], torch.LongTensor(test_data[0]))\n",
    "# # testData = TensorDataset(torch.Tensor(test_data[2]), torch.LongTensor(test_data[0]))\n",
    "# print('to tensor done')\n",
    "\n",
    "# testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "# print('dataloader done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to tensor done\n",
      "dataloader done\n"
     ]
    }
   ],
   "source": [
    "# testData = TensorDataset(torch.Tensor(test_data[2]), torch.LongTensor(test_data[0]))\n",
    "testData = TensorDataset((torch.Tensor(test_data[2]))[:, :10], torch.LongTensor(test_data[0]))\n",
    "print('to tensor done')\n",
    "\n",
    "testLoader = DataLoader(dataset=testData, batch_size=1, shuffle=False)\n",
    "print('dataloader done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*2*2*10, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for k in range(10):\n",
    "            f = x[:,k,:,:,:]\n",
    "            f = f.permute(0,3,1,2) \n",
    "            f = self.features(f)\n",
    "            f = f.view(f.size(0), 256 * 2 * 2)\n",
    "            out.append(f)\n",
    "        out = torch.cat(out, axis=1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[265.0, 29.0, 56.0, 1044.0, 313.0, 96.0, 512.0]\n",
      "[329.0, 74.0, 160.0, 1185.0, 478.0, 162.0, 680.0]\n",
      "0.8054711246200608\n",
      "0.3918918918918919\n",
      "0.35\n",
      "0.8810126582278481\n",
      "0.6548117154811716\n",
      "0.5925925925925926\n",
      "0.7529411764705882\n",
      "Accuracy: 0.754563\tclassID:1\tacc:0.805471\t\tclassID:2\tacc:0.391892\t\tclassID:3\tacc:0.350000\t\tclassID:4\tacc:0.881013\t\tclassID:5\tacc:0.654812\t\tclassID:6\tacc:0.592593\t\tclassID:7\tacc:0.752941\t\n"
     ]
    }
   ],
   "source": [
    "vgg16 = VGG16()\n",
    "# vgg16 = torch.nn.DataParallel(vgg16)\n",
    "\n",
    "path = './RAFDB/alexnetnewepoch169.pkl'\n",
    "from collections import OrderedDict\n",
    "state_dictBA = torch.load(path)['net']\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "new_state_dictBA = OrderedDict()\n",
    "for k, v in state_dictBA.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dictBA[name] = v\n",
    "vgg16.load_state_dict(new_state_dictBA)\n",
    "# checkpoint = torch.load(path)\n",
    "\n",
    "# vgg16.load_state_dict(checkpoint['net'])\n",
    "vgg16.cuda()\n",
    "\n",
    "# Test the model\n",
    "vgg16.eval()\n",
    "\n",
    "correct = list(0. for i in range(7))\n",
    "total = list(0. for i in range(7))\n",
    "suprise = list(0. for i in range(7))\n",
    "fear = list(0. for i in range(7))\n",
    "disgust = list(0. for i in range(7))\n",
    "happy = list(0. for i in range(7))\n",
    "sad = list(0. for i in range(7))\n",
    "anger = list(0. for i in range(7))\n",
    "netual = list(0. for i in range(7))\n",
    "for i, (images, labels) in enumerate(testLoader):\n",
    "    images = Variable(images.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "\n",
    "    output = vgg16(images)\n",
    "\n",
    "    prediction = torch.argmax(output, 1)\n",
    "    res = prediction == labels\n",
    "    \n",
    "    for label_idx in range(len(labels)):  \n",
    "        pre = prediction[label_idx]\n",
    "        label_single = labels[label_idx]\n",
    "        if label_single == 0:\n",
    "            suprise[pre] += 1\n",
    "        elif label_single == 1:\n",
    "            fear[pre] += 1\n",
    "        elif label_single == 2:\n",
    "            disgust[pre] += 1\n",
    "        elif label_single == 3:\n",
    "            happy[pre] += 1\n",
    "        elif label_single == 4:\n",
    "            sad[pre] += 1\n",
    "        elif label_single == 5:\n",
    "            anger[pre] += 1\n",
    "        elif label_single == 6:\n",
    "            netual[pre] += 1\n",
    "        \n",
    "        correct[label_single] += res[label_idx].item()\n",
    "        total[label_single] += 1\n",
    "acc_str = 'Accuracy: %f'%(sum(correct)/sum(total))\n",
    "print(correct)\n",
    "print(total)\n",
    "for acc_idx in range(7):\n",
    "    try:\n",
    "        acc = correct[acc_idx]/total[acc_idx]\n",
    "        print(acc)\n",
    "    except:\n",
    "        acc = 0\n",
    "    finally:\n",
    "        acc_str += '\\tclassID:%d\\tacc:%f\\t'%(acc_idx+1, acc)\n",
    "print(acc_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.0, 3.0, 15.0, 68.0, 56.0, 4.0, 512.0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6326744513263076"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alexnet \n",
    "(0.8054711246200608+0.3918918918918919+0.35+0.8810126582278481+0.6548117154811716+0.5925925925925926+0.7529411764705882)/7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6326744513263076"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8054711246200608+0.3918918918918919+0.35+0.8810126582278481+0.6548117154811716+0.5925925925925926+0.7529411764705882)/7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1gpu_40Gmemory_8cpu",
   "language": "python3",
   "name": "1gpu_40gmemory_8cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
